{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mpfmorawski/pyconpl2023-speech-recognition/blob/main/pyconopl2023_speech_recognition_with_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elyQ65vnoLbI"
      },
      "source": [
        "# Content\n",
        "During today's lecture we will test the following solutions:\n",
        "1. [SpeechRecognition](https://github.com/Uberi/speech_recognition) (Python module supporting several speech-to-text engines and APIs)\n",
        "2. [AssemblyAI](https://www.assemblyai.com/) (API)\n",
        "3. [Whisper](https://github.com/openai/whisper) (speech-to-text model)\n",
        "4. [Transformers](https://github.com/huggingface/transformers) (pretrained speech-to-text models)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5Cl0mxn2Ufr"
      },
      "source": [
        "# Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RMQV5HhlDj6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b97cBmQK2YLr"
      },
      "outputs": [],
      "source": [
        "!pip install contexttimer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjIPQR5G2ZTw"
      },
      "outputs": [],
      "source": [
        "import contexttimer\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlGmmQbYxyP6"
      },
      "source": [
        "# Audio files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the audio files from [here](https://drive.google.com/drive/folders/1i-F-dVNvvMBG2TJEO2boT-3ihXnIRT4l?usp=sharing) or download them from the [repository](https://github.com/mpfmorawski/pyconpl2023-speech-recognition) and upload them to your personal Google Drive.\n",
        "\n",
        "And then update the following PATH variable:"
      ],
      "metadata": {
        "id": "L5ZWZJcfCit0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"drive/MyDrive/PUT_YOUR_FOLDER_NAME_HERE\""
      ],
      "metadata": {
        "id": "5gNRkH8bCWfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CpCTqzZzorY"
      },
      "outputs": [],
      "source": [
        "import IPython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zJQb3Vrxuzs"
      },
      "outputs": [],
      "source": [
        "sentence = {\"en\": f\"{PATH}/sentence_en.wav\",\n",
        "            \"pl\": f\"{PATH}/sentence_pl.wav\"}\n",
        "\n",
        "command = {\"en\": f\"{PATH}/command_en.wav\",\n",
        "           \"pl\": f\"{PATH}/command_pl.wav\"}\n",
        "\n",
        "def display_audio_example_in_all_languages(example: dict) -> None:\n",
        "  for language in example:\n",
        "    print(language)\n",
        "    IPython.display.display(IPython.display.Audio(example[language]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Long sentence\")\n",
        "display_audio_example_in_all_languages(sentence)\n",
        "print(\"\\nShort command\")\n",
        "display_audio_example_in_all_languages(command)"
      ],
      "metadata": {
        "id": "Yh5IiXTGCiYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKX4fZ04o_dg"
      },
      "source": [
        "# 1 - SpeechRecognition\n",
        "[Speech recognition module](https://github.com/Uberi/speech_recognition) *for Python, supporting several engines and APIs, online and offline.*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Speech recognition engine/API support (based on its [README](https://github.com/Uberi/speech_recognition/blob/master/README.rst)):\n",
        "\n",
        "* [CMU Sphinx](http://cmusphinx.sourceforge.net/wiki/) (works offline)\n",
        "* Google Speech Recognition\n",
        "* [Google Cloud Speech API](https://cloud.google.com/speech/)\n",
        "* [Wit.ai](https://wit.ai/)\n",
        "* [Microsoft Azure Speech](https://azure.microsoft.com/en-us/services/cognitive-services/speech)\n",
        "* [Houndify API](https://houndify.com/)\n",
        "* [IBM Speech to Text](http://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/speech-to-text.html)\n",
        "* [Snowboy Hotword Detection](https://snowboy.kitt.ai/) (works offline)\n",
        "* [Tensorflow](https://www.tensorflow.org/)\n",
        "* [Vosk API](https://github.com/alphacep/vosk-api/) (works offline)\n",
        "* [OpenAI whisper](https://github.com/openai/whisper) (works offline)\n",
        "* [Whisper API](https://platform.openai.com/docs/guides/speech-to-text)"
      ],
      "metadata": {
        "id": "kE-YCrMMBdor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation and import"
      ],
      "metadata": {
        "id": "Ceb-_jexDEC1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRWgUaiYwskj"
      },
      "outputs": [],
      "source": [
        "!pip install SpeechRecognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PnpPovMsp5V"
      },
      "outputs": [],
      "source": [
        "import speech_recognition as sr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5RFmb3YvXhW"
      },
      "source": [
        "## Speech recognition using speech_recognition module with Google Speech Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXWTprdzud_-"
      },
      "outputs": [],
      "source": [
        "def transcribe_with_speech_recognition_module_and_google(audio_path: str) -> str:\n",
        "  r = sr.Recognizer()\n",
        "  with sr.AudioFile(audio_path) as source:\n",
        "      audio = r.record(source)\n",
        "\n",
        "  try:\n",
        "      return r.recognize_google(audio)\n",
        "  except sr.UnknownValueError:\n",
        "      return \"Could not understand audio\"\n",
        "  except sr.RequestError as e:\n",
        "      return f\"Could not request results from service; {e}\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code source: https://github.com/Uberi/speech_recognition/blob/master/examples/audio_transcribe.py\n"
      ],
      "metadata": {
        "id": "jbjRbs-paOQL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBTqDjP7vs8o"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaQmHuuK55m6"
      },
      "outputs": [],
      "source": [
        "audio_path = sentence[\"en\"]\n",
        "\n",
        "print(f\"Audio file: {audio_path}\")\n",
        "IPython.display.display(IPython.display.Audio(audio_path))\n",
        "\n",
        "with contexttimer.Timer() as t:\n",
        "  transcription = transcribe_with_speech_recognition_module_and_google(audio_path)\n",
        "\n",
        "print(f\"\\nReceived transcription:\\n\\n{transcription} \\n\\nExecution time: {t.elapsed:.2f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxNV7sDp270J"
      },
      "outputs": [],
      "source": [
        "audio_path = command[\"en\"]\n",
        "\n",
        "print(f\"Audio file: {audio_path}\")\n",
        "IPython.display.display(IPython.display.Audio(audio_path))\n",
        "\n",
        "with contexttimer.Timer() as t:\n",
        "  transcription = transcribe_with_speech_recognition_module_and_google(audio_path)\n",
        "\n",
        "print(f\"\\nReceived transcription:\\n\\n{transcription} \\n\\nExecution time: {t.elapsed:.2f} s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZRWavue1Ued"
      },
      "source": [
        "## Usage of other speech recognition engines and APIs\n",
        "You can find speech_recognition module usage with other engines and APIs examples here https://github.com/Uberi/speech_recognition/blob/master/examples/audio_transcribe.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADHFWaVEo1l1"
      },
      "source": [
        "# 2 - AssemblyAI\n",
        "[AssemblyAI](https://www.assemblyai.com/) *API exposes AI models for speech recognition, speaker detection, speech summarization, and more.*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "FYTP7hXrGP1u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRoONeZP4-J3"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulUqFOV3mrAL"
      },
      "source": [
        "## Code template\n",
        "Getting *Try the API* code from main page of https://www.assemblyai.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESo9bn0WnSLO"
      },
      "outputs": [],
      "source": [
        "endpoint = \"https://api.assemblyai.com/v2/transcript\"\n",
        "\n",
        "json = {\n",
        "  \"audio_url\": \"https://storage.googleapis.com/bucket/b2c31290d9d8.wav\"\n",
        "}\n",
        "\n",
        "headers = {\n",
        "  \"Authorization\": \"c2a41970d9d811ec9d640242ac12\",\n",
        "  \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.post(endpoint, json=json, headers=headers)\n",
        "parse(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2Oe0ZMdngWi"
      },
      "source": [
        "Code analysis:\n",
        "1. Audio files were uploaded to web!\n",
        "2. Need to get your own AssemblyAI API Key\n",
        "3. Need to analyze what data comes in response (parsing)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authorization"
      ],
      "metadata": {
        "id": "ky3NuOVTLWNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Go to: https://www.assemblyai.com/dashboard/signup\n",
        "2. Sign up\n",
        "3. Go to: https://www.assemblyai.com/app/account\n",
        "4. Copy your API Key"
      ],
      "metadata": {
        "id": "IhZPq9pbLrFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ASSEMBLY_AI_API_KEY = \"PUT_YOUR_ASSEMBLY_AI_API_KEY_HERE\""
      ],
      "metadata": {
        "id": "ndi5aBGrMbJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o82HL7ViphnJ"
      },
      "source": [
        "## Uploading audio files\n",
        "Uploading files for transcription basing on https://www.assemblyai.com/docs/walkthroughs#uploading-local-files-for-transcription"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = sentence[\"en\"]"
      ],
      "metadata": {
        "id": "apVbYp_6GuNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOMsmqZerOe2"
      },
      "outputs": [],
      "source": [
        "UPLOAD_ENDPOINT = \"https://api.assemblyai.com/v2/upload\"\n",
        "headers = {\"authorization\": ASSEMBLY_AI_API_KEY}\n",
        "with open(filename , \"rb\") as f:\n",
        "    response = requests.post(UPLOAD_ENDPOINT,\n",
        "                        headers=headers,\n",
        "                        data=f)\n",
        "\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diQJBEf2sp1a"
      },
      "outputs": [],
      "source": [
        "TRANSCRIPT_ENDPOINT = \"https://api.assemblyai.com/v2/transcript\"\n",
        "\n",
        "json = {\n",
        "    \"audio_url\": response.json()[\"upload_url\"]\n",
        "}\n",
        "headers = {\n",
        "    \"authorization\": ASSEMBLY_AI_API_KEY,\n",
        "}\n",
        "\n",
        "response = requests.post(TRANSCRIPT_ENDPOINT,\n",
        "                         json=json,\n",
        "                         headers=headers)\n",
        "\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtXJeqm_uy68"
      },
      "source": [
        "But wait... where is transcription?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CljdfWVtEow"
      },
      "outputs": [],
      "source": [
        "print(f\"{response.json()['text']=}\")\n",
        "print(f\"{response.json()['status']=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiB6SrLUtFJf"
      },
      "source": [
        "## Polling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "polling_endpoint = f\"{TRANSCRIPT_ENDPOINT}/{response.json()['id']}\"\n",
        "\n",
        "while True:\n",
        "  response = requests.get(polling_endpoint, headers=headers).json()\n",
        "  if response[\"status\"] == \"completed\":\n",
        "    break\n",
        "  elif response[\"status\"] == \"error\":\n",
        "    raise RuntimeError(f\"Transcription failed: {response['error']}\")\n",
        "  else:\n",
        "    time.sleep(3)\n",
        "\n",
        "print(response[\"text\"])"
      ],
      "metadata": {
        "id": "RbFOH69nb-p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## All components together\n"
      ],
      "metadata": {
        "id": "W0XJtTPwb_O4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhqrALkW-tVD"
      },
      "outputs": [],
      "source": [
        "UPLOAD_ENDPOINT = \"https://api.assemblyai.com/v2/upload\"\n",
        "TRANSCRIPT_ENDPOINT = \"https://api.assemblyai.com/v2/transcript\"\n",
        "\n",
        "headers = {\"authorization\": ASSEMBLY_AI_API_KEY}\n",
        "\n",
        "def upload_audio_file(filename : str) -> str:\n",
        "  with open(filename , \"rb\") as f:\n",
        "    response = requests.post(UPLOAD_ENDPOINT,\n",
        "                        headers=headers,\n",
        "                        data=f)\n",
        "  return response.json()[\"upload_url\"]\n",
        "\n",
        "\n",
        "def make_transcription_request(audio_url) -> str:\n",
        "  json = { \"audio_url\": audio_url }\n",
        "  response = requests.post(TRANSCRIPT_ENDPOINT, json=json, headers=headers)\n",
        "  return response.json()[\"id\"]\n",
        "\n",
        "\n",
        "def poll(transcript_id):\n",
        "  polling_endpoint = f\"{TRANSCRIPT_ENDPOINT}/{transcript_id}\"\n",
        "  polling_response = requests.get(polling_endpoint, headers=headers)\n",
        "  return polling_response.json()\n",
        "\n",
        "\n",
        "def transcribe_with_assembly_ai(audio_path: str):\n",
        "  audio_url = upload_audio_file(audio_path)\n",
        "  transcription_id = make_transcription_request(audio_url)\n",
        "  while True:\n",
        "    response = requests.get(f\"{TRANSCRIPT_ENDPOINT}/{transcription_id}\", headers=headers).json()\n",
        "    if response[\"status\"] == \"completed\":\n",
        "      return response[\"text\"]\n",
        "    elif response[\"status\"] == \"error\":\n",
        "      raise RuntimeError(f\"Transcription failed: {response['error']}\")\n",
        "    else:\n",
        "      time.sleep(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code source: https://www.assemblyai.com/docs/walkthroughs#uploading-local-files-for-transcription"
      ],
      "metadata": {
        "id": "nX6NfRYCcHpD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsmfE9XDvIFr"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v57HlJTK_Ert"
      },
      "outputs": [],
      "source": [
        "audio_path = sentence[\"en\"]\n",
        "\n",
        "print(f\"Audio file: {audio_path}\")\n",
        "IPython.display.display(IPython.display.Audio(audio_path))\n",
        "\n",
        "with contexttimer.Timer() as t:\n",
        "  transcription = transcribe_with_assembly_ai(audio_path)\n",
        "\n",
        "print(f\"\\nReceived transcription:\\n\\n{transcription} \\n\\nExecution time: {t.elapsed:.2f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbTD10PI0FtO"
      },
      "outputs": [],
      "source": [
        "audio_path = command[\"en\"]\n",
        "\n",
        "print(f\"Audio file: {audio_path}\")\n",
        "IPython.display.display(IPython.display.Audio(audio_path))\n",
        "\n",
        "with contexttimer.Timer() as t:\n",
        "  transcription = transcribe_with_assembly_ai(audio_path)\n",
        "\n",
        "print(f\"\\nReceived transcription:\\n\\n{transcription} \\n\\nExecution time: {t.elapsed:.2f} s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - OpenAI's Whisper\n",
        "*Robust Speech Recognition via Large-Scale Weak Supervision*"
      ],
      "metadata": {
        "id": "QUTLNDtqkkJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation and import"
      ],
      "metadata": {
        "id": "kJWjiRNiknT-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij_qZbWGrH15"
      },
      "outputs": [],
      "source": [
        "!pip install -U openai-whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7Yg6FhKBd8q"
      },
      "outputs": [],
      "source": [
        "import whisper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6ouH5VFNeCu"
      },
      "source": [
        "## Importing models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u2js6F4NjVO"
      },
      "outputs": [],
      "source": [
        "english_only_models_names = [\"tiny.en\", \"base.en\", \"small.en\"]\n",
        "# Note: You can use a 'medium.en' model too but it is quite big (1.42G) - Google Colab sometimes crashes because of it\n",
        "multilingual_models_names = [\"tiny\", \"base\", \"small\"]\n",
        "# Note: You can use a 'medium' model and a 'large' medels too but...\n",
        "# A \"medium\" model is quite big (1.42G) - Google Colab sometimes crashes because of it\n",
        "# A \"large\" model is too large to even import it in google colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzT0QRtPM52G"
      },
      "source": [
        "### Importing English-only models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dO_Ei3wEpIx"
      },
      "outputs": [],
      "source": [
        "english_only_models = [whisper.load_model(model_name) for model_name in english_only_models_names]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygUYqi4ANGfa"
      },
      "source": [
        "### Importing multilingual models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odZqPWI6EvKr"
      },
      "outputs": [],
      "source": [
        "multilingual_models = [whisper.load_model(model_name) for model_name in multilingual_models_names]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdOWWEo0_2U7"
      },
      "source": [
        "## Speech recognition with OpenAI's Whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQMPCBc65BbH"
      },
      "outputs": [],
      "source": [
        "def transcribe_with_whipser(model, audio_path: str) -> dict:\n",
        "  return model.transcribe(audio_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code source: https://github.com/openai/whisper/blob/main/notebooks/Multilingual_ASR.ipynb"
      ],
      "metadata": {
        "id": "QYqxcd2aFGzc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geMMCCE57BuM"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wox6V17JWlj1"
      },
      "source": [
        "### Testing English-only models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85KbZizA7R0s"
      },
      "outputs": [],
      "source": [
        "def test_english_only_models(audio_path: str) -> None:\n",
        "  for index, model_name in enumerate(english_only_models_names):\n",
        "    with contexttimer.Timer() as t:\n",
        "      result = transcribe_with_whipser(english_only_models[index], audio_path)\n",
        "    print(f\"Model: {model_name}\\nReceived transcription: {result['text']} | Detected language: {result['language']} | Execution time: {t.elapsed:.2f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDjiQ_5PNzmd"
      },
      "outputs": [],
      "source": [
        "audio_path = sentence[\"en\"]\n",
        "\n",
        "print(f\"Audio file: {audio_path}\")\n",
        "IPython.display.display(IPython.display.Audio(audio_path))\n",
        "\n",
        "test_english_only_models(audio_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGMg1_B770Jz"
      },
      "outputs": [],
      "source": [
        "audio_path = command[\"en\"]\n",
        "\n",
        "print(f\"Audio file: {audio_path}\")\n",
        "IPython.display.display(IPython.display.Audio(audio_path))\n",
        "\n",
        "test_english_only_models(audio_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4OqTKCLNuSr"
      },
      "source": [
        "### Testing multilingual base model with different languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5csPTYi8aqn"
      },
      "outputs": [],
      "source": [
        "def test_multilingual_base_model(audio_path: str) -> None:\n",
        "  base_model_index = 1\n",
        "  with contexttimer.Timer() as t:\n",
        "    result = transcribe_with_whipser(multilingual_models[base_model_index], audio_path)\n",
        "  print(f\"Model: base | Received transcription: {result['text']} | Detected language: {result['language']} | Execution time: {t.elapsed:.2f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXp0WSEx9OzQ"
      },
      "outputs": [],
      "source": [
        "audio_path = sentence[\"en\"]\n",
        "\n",
        "print(f\"Audio file: {audio_path}\")\n",
        "IPython.display.display(IPython.display.Audio(audio_path))\n",
        "\n",
        "test_multilingual_base_model(audio_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTCWPmnF_xlh"
      },
      "outputs": [],
      "source": [
        "audio_path = sentence[\"pl\"]\n",
        "\n",
        "print(f\"Audio file: {audio_path}\")\n",
        "IPython.display.display(IPython.display.Audio(audio_path))\n",
        "\n",
        "test_multilingual_base_model(audio_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMcYUBy7__Uh"
      },
      "outputs": [],
      "source": [
        "audio_path = command[\"en\"]\n",
        "\n",
        "print(f\"Audio file: {audio_path}\")\n",
        "IPython.display.display(IPython.display.Audio(audio_path))\n",
        "\n",
        "test_multilingual_base_model(audio_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVeESh6tADq7"
      },
      "outputs": [],
      "source": [
        "audio_path = command[\"pl\"]\n",
        "\n",
        "print(f\"Audio file: {audio_path}\")\n",
        "IPython.display.display(IPython.display.Audio(audio_path))\n",
        "\n",
        "test_multilingual_base_model(audio_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Transformers\n",
        "*Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.*"
      ],
      "metadata": {
        "id": "GKKh73q7uWvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation and import"
      ],
      "metadata": {
        "id": "VCpFrWtyk6D7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJqj7078lCvT"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3VkzHvAk3G0"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speech recognition with Transformers"
      ],
      "metadata": {
        "id": "VjY96ELQlDZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_with_transformers_pipeline(audio_path: str) -> str:\n",
        "  transcriber = pipeline(\"automatic-speech-recognition\",\n",
        "                         model=\"facebook/wav2vec2-base-960h\")\n",
        "  transcription = transcriber(audio_path)\n",
        "  return transcription[\"text\"]"
      ],
      "metadata": {
        "id": "4Qvu_mHLiSFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code source: https://huggingface.co/docs/transformers/main/tasks/asr"
      ],
      "metadata": {
        "id": "bLO-YWYhmzPU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDZd_G3xi2Wo"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlWm2thNi2Wu"
      },
      "outputs": [],
      "source": [
        "audio_path = sentence[\"en\"]\n",
        "\n",
        "print(f\"Audio file: {audio_path}\")\n",
        "IPython.display.display(IPython.display.Audio(audio_path))\n",
        "\n",
        "with contexttimer.Timer() as t:\n",
        "  transcription = transcribe_with_transformers_pipeline(audio_path)\n",
        "\n",
        "print(f\"\\nReceived transcription:\\n\\n{transcription} \\n\\nExecution time: {t.elapsed:.2f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6tT8UP6i2Wu"
      },
      "outputs": [],
      "source": [
        "audio_path = command[\"en\"]\n",
        "\n",
        "print(f\"Audio file: {audio_path}\")\n",
        "IPython.display.display(IPython.display.Audio(audio_path))\n",
        "\n",
        "with contexttimer.Timer() as t:\n",
        "  transcription = transcribe_with_transformers_pipeline(audio_path)\n",
        "\n",
        "print(f\"\\nReceived transcription:\\n\\n{transcription} \\n\\nExecution time: {t.elapsed:.2f} s\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "O9KkCFLTKcJy"
      ],
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1mi1UcoTzwgpUGhUX0o_2qpRkXCzhHFdo",
      "authorship_tag": "ABX9TyPZImYyiJfEkcfpJNHpgelj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}